{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Prerequisite Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cluster import Birch\n",
    "import requests\n",
    "import json\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data_tr.txt',sep='\\t',header=None)\n",
    "actual_data=pd.read_csv('gene_names.txt',header=None)\n",
    "\n",
    "labels = actual_data[0].tolist()\n",
    "\n",
    "training_data.columns = labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Standard Deviation we drop all the features with STD < 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features dropped  4994\n"
     ]
    }
   ],
   "source": [
    "genes = []\n",
    "std_deviations = []\n",
    "for (col_name, col_data) in training_data.iteritems():\n",
    "    if(np.std(col_data) <= 0.10):\n",
    "        genes.append(col_name)\n",
    "        std_deviations.append(np.std(col_data))  \n",
    "training_data.drop(genes, axis=1, inplace= True)\n",
    "\n",
    "print(\"No. of features dropped \",len(genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Training Data into 100 equal parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection = np.array_split(training_data, 100, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over the samples and dropping features with Correlation > 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  1\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  1\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  1\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  1\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  1\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n",
      "Features dropped  0\n"
     ]
    }
   ],
   "source": [
    "for i in df_collection:\n",
    "    corr_matrix = i.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.60)]\n",
    "    training_data.drop(to_drop, axis=1, inplace=True)\n",
    "    print(\"Features dropped \",len(to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Variance Threshold and Dropping the Quasi Constant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038, 13177)\n"
     ]
    }
   ],
   "source": [
    "variance_filter = VarianceThreshold(threshold=0.1)\n",
    "new_data = variance_filter.fit(training_data)\n",
    "\n",
    "qconstant_columns = [column for column in training_data.columns\n",
    "                    if column not in training_data.columns[variance_filter.get_support()]]\n",
    "\n",
    "new_data = training_data.drop(qconstant_columns,axis = 1)\n",
    "training_data = new_data\n",
    "\n",
    "new_data_T = new_data.T\n",
    "print(new_data_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Correlation Again, this time to the Full Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = training_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.append(colname)\n",
    "            \n",
    "training_data.drop(correlated_features, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final list of Features  1025\n"
     ]
    }
   ],
   "source": [
    "final_features = len(training_data.columns)\n",
    "print(\"Final list of Features \",final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling / Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(training_data)\n",
    "scaled_features_train = scaler.transform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=45,svd_solver='full')\n",
    "fitted_data = pca.fit_transform(scaled_features_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally saving these features to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv('final_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are now ready to use this data to train and test our models for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11152430940105786\n"
     ]
    }
   ],
   "source": [
    "model = KMeans(n_clusters=16, random_state=0, init=\"k-means++\")\n",
    "model.fit(fitted_data)\n",
    "predictions = model.predict(fitted_data)\n",
    "kmeans_silhouette = silhouette_score(fitted_data, model.labels_)\n",
    "print(kmeans_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1490037516769233\n"
     ]
    }
   ],
   "source": [
    "# Creating the BIRCH clustering model\n",
    "model = Birch(n_clusters = 16)\n",
    "  \n",
    "# Fit the data (Training)\n",
    "model.fit(fitted_data)\n",
    "  \n",
    "# Predict the same data\n",
    "predictions = model.predict(fitted_data)\n",
    "\n",
    "birch_silhouette = silhouette_score(fitted_data, model.labels_)\n",
    "print(birch_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1566103581508076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=16,assign_labels='discretize',random_state=0).fit_predict(fitted_data)\n",
    "\n",
    "spec_silhouette = silhouette_score(fitted_data, model)\n",
    "print(spec_silhouette)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
